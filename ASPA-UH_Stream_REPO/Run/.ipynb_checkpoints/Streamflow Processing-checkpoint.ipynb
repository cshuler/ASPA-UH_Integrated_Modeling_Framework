{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASPA-UH streamflow data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readme\n",
    "\n",
    "This repository is the cloud-home of the UH-ASPA stream gauge project, which is currently mainteined by C. Shuler (UH) and M. Erickson (ASPA) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Notes about Base Data. \n",
    "The script uses two supplimentary datasets that can be added to if additional measurements are taken (so that rating curves can self-update) or if the stream gauge housing has a change in elevation, either from being physically moved or if the object it is mounted to is offset somehow ( so that the script can automatically correct for any known movements in sensor elevation). It is imperitive that these datasets are kept consistent and up to date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " <style>    div#notebook-container    { width: 85%; }    div#menubar-container    \n",
       "{ width: 85%; }    div#maintoolbar-container { width: 99%; } </style> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import all libraries (ensure all are listed in the environment.yml file)\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "def datetime_range(start, end, delta):\n",
    "    current = start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Which Stations to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily flow totalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")  # close previous figures to clear memory\n",
    "# this subsamples the data into daily values\n",
    "\n",
    "Curve_type =   '2d_poly_fit_Modeled_Q'    # 'Man_q_m3/sec'    or    'Power_law_Modeled_Q'       or        '2d_poly_fit_Modeled_Q'\n",
    "\n",
    "master_day_flow = {}\n",
    "for i in stations:\n",
    "\n",
    "    day_flow = All_masters_adjusted[i].set_index('Date and Time').resample('D').mean() \n",
    "    day_flow = day_flow.reset_index(drop=False)    \n",
    "    master_day_flow[i] = day_flow\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    ax1 = plt.axes()\n",
    "    ax1.set_title(i, color='darkblue')\n",
    "    plt.plot(All_masters_adjusted[i]['Date and Time'], All_masters_adjusted[i][Curve_type], alpha = .7, color='b',  marker = '.', label = \"15 minute flows [m3/s]\")\n",
    "    plt.plot(day_flow['Date and Time'], day_flow[Curve_type] , alpha = .7, color='y',  marker = '.', label = \"Daily average flows [m3/s]\")\n",
    "    ax1.legend(loc=2)\n",
    "\n",
    "    plt.xticks(rotation=20)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "Average_daily_flows = pd.DataFrame({'Date and Time':day_flow['Date and Time']}) # create empty dtaframe with random date column from above\n",
    "Raws ={}                                                                        # duplicate daa storage formatting for baseflow separation below\n",
    "\n",
    "for i in stations: \n",
    "    Qs = master_day_flow[i][['Date and Time', Curve_type]].copy()\n",
    "    keynam = i+\"_CFS\"\n",
    "    Qs[keynam] = Qs[Curve_type]*35.314666212661                    # convert flow in m3/s to cfs\n",
    "    del Qs[Curve_type]\n",
    "    Average_daily_flows = Average_daily_flows.merge(Qs, how='outer', on='Date and Time')\n",
    "    print(\"{} ave flow {} CFS\".format(i, Qs[keynam].mean()))\n",
    "         \n",
    "    Qs = Qs.rename(index=str, columns={\"Date and Time\": \"Date\", keynam: \"Total Flow (cfs)\"})\n",
    "    Raws[i] = Qs                                                               # duplicate daa storage formatting for baseflow separation below\n",
    "\n",
    "Average_daily_flows = Average_daily_flows.sort_values('Date and Time')  # here is the sorted daily streamflow dataframe from all stations lined up on date NOTE USING Curve_type FROM ABOVE can change if want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseflow separation \n",
    "\n",
    "Note this uses the Turning Point method, described in \"U.S. Geological Survey Groundwater Toolbox, A Graphical and Mapping Interface for Analysis of Hydrologic Data (Version 1.0)—User Guide for Estimation of Base Flow, Runoff, and Groundwater Recharge From Streamflow Data,  \n",
    "Pg. 2 explains the process and cites The BFI program (Wahl and Wahl, 1995) as the method used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")  # close previous figures to clear memory\n",
    "\n",
    "N = 5                     # averaging window, number of days\n",
    "tp_test_factor = 0.9      # turning point test factor     (If 90 percent of a given minimum (the “turning point test factor”) is less than both adjacent minimums, then that minimum is a turning point.)\n",
    "\n",
    "All_stations = {}                                                                           # eill be final processed dictionary of dataframes\n",
    "Site=[]; SumTotal=[]; SumBF=[]; SumRO=[]; AveTotal=[]; AveBF=[]; AveRO=[]; BFTF=[]; ROTF=[]  # lists for sumary dataframe\n",
    "\n",
    "for stato in stations:\n",
    "    mean_dates = []; mins = []; means = []\n",
    "    for i in range(1,len(Raws[stato]['Date'])-N,N):                         # mikes code, still not sure I understand it all\n",
    "        N_day_data = []\n",
    "        for j in range(0,N-1):\n",
    "            N_day_data.append(float(Raws[stato]['Total Flow (cfs)'][i+j]))\n",
    "        mean_dates.append(Raws[stato]['Date'][i+(N//2)])\n",
    "        mean_point = [Raws[stato]['Date'][i+(N//2)]]\n",
    "        N_day_mean = np.mean(N_day_data)\n",
    "        mean_point.append(N_day_mean)\n",
    "        means.append(mean_point)\n",
    "        min_point = [Raws[stato]['Date'][i+(N//2)]]\n",
    "        N_day_min = np.min(N_day_data)\n",
    "        min_point.append(N_day_min)\n",
    "        mins.append(min_point)\n",
    "\n",
    "    turning_points = []; tp_dates = []; tp_flow = []; sf_dates = []; sf_flow = [];     # mikes code, still not sure I understand it all\n",
    "    for i in range(0,len(mins)-1,1):\n",
    "        if (tp_test_factor*(mins[i][1]))<mins[i+1][1] and (tp_test_factor*(mins[i][1]))<mins[i-1][1]:\n",
    "            turning_points.append(mins[i])\n",
    "            tp_dates.append(mins[i][0])\n",
    "            tp_flow.append(mins[i][1])\n",
    "\n",
    "    Total_flows = pd.DataFrame({'Date': Raws[stato]['Date'], 'Total_flow_CFS': Raws[stato]['Total Flow (cfs)'] })\n",
    "    Baseflows = pd.DataFrame({'Date': tp_dates, 'Base_flow_CFS': tp_flow })  \n",
    "    All_flows = Total_flows.merge(Baseflows, how='outer', on='Date')                     # final dataframe with separated values of flow\n",
    "    All_flows['Base_flow_CFS'].interpolate(inplace=True)                                 # baseflows were only calculated at turning points. here linearly interpolate to give a value for each day\n",
    "    All_flows['Runoff_CFS'] = All_flows['Total_flow_CFS'] - All_flows['Base_flow_CFS']\n",
    "    All_flows['Runoff_CFS'] = All_flows['Runoff_CFS'].clip(lower=0)    # convert any negative runoff values to zero\n",
    "    \n",
    "    All_stations[stato] = All_flows                      # create final dictionary                            \n",
    "    \n",
    "    # all this to create the summary dataframe for comparison\n",
    "    Site.append(stato); SumTotal.append(All_flows['Total_flow_CFS'].sum()); SumBF.append(All_flows['Base_flow_CFS'].sum()); SumRO.append(All_flows['Total_flow_CFS'].sum()-All_flows['Base_flow_CFS'].sum())\n",
    "    AveTotal.append(All_flows['Total_flow_CFS'].mean()); AveBF.append(All_flows['Base_flow_CFS'].mean()); AveRO.append(All_flows['Total_flow_CFS'].mean()-All_flows['Base_flow_CFS'].mean())\n",
    "    BFTF.append((All_flows['Base_flow_CFS'].mean()/All_flows['Total_flow_CFS'].mean()))\n",
    "    \n",
    "    # plot stuff\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.plot(All_flows['Date'],All_flows['Total_flow_CFS'], '-',label='Total daily flow', marker='.')\n",
    "    ax.plot(All_flows['Date'],All_flows['Base_flow_CFS'], '-',label='Baseflow', marker='.')\n",
    "    ax.set_title(stato)\n",
    "    ax.legend()\n",
    "    plt.ylabel('Discharge (CFS)')\n",
    "    \n",
    "    plt.xticks(rotation=20)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### produce streamflow statistics, yearly, and monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the annual summary dataframe\n",
    "Annual_stats_all = pd.DataFrame({'Site':Site,'Ave_Total_Q_[cfs]':AveTotal,'AveBF_[cfs]':AveBF,'Ave RO_[cfs]':AveRO,'BF:Q':BFTF})\n",
    "\n",
    "# throw a csv of the annual stats up on the desktop\n",
    "Annual_stats_all.to_csv(os.path.join(workspace, \"Annual_UH-ASPA_streamflows.csv\"))\n",
    "\n",
    "# Consolidate by a dictionary of stations {Station_stats} with an entry for each month\n",
    "Station_stats ={}\n",
    "for stato in stations:\n",
    "    All_stations[stato]['Month'] = All_stations[stato]['Date'].apply(lambda i: i.month)\n",
    "    M = []; TF_sum = []; BF_sum =[]; RO_sum = []\n",
    "    for i in All_stations[stato]['Month'].unique():\n",
    "        a = All_stations[stato][All_stations[stato]['Month'] == i]\n",
    "        tf = a['Total_flow_CFS'].mean()\n",
    "        bf = a['Base_flow_CFS'].mean()\n",
    "        ro = a['Runoff_CFS'].mean()\n",
    "        M.append(i); TF_sum.append(tf); BF_sum.append(bf); RO_sum.append(ro)\n",
    "    tica  = pd.DataFrame({'Month':M,'Total_flow':TF_sum,'Baseflow':BF_sum, 'Runoff':RO_sum,})\n",
    "    tica['BF:TF'] = tica['Baseflow']/tica['Total_flow']\n",
    "    tica['Site'] = stato\n",
    "    Station_stats[stato] = tica\n",
    "    \n",
    "# Consolidate by a dictionary of months  {Monthly_stats} with an entry for each station\n",
    "Monthly_stats = {}\n",
    "month_key = {1:'January',  2:'February',  3:'March',  4:'April',  5:'May',  6:'June',  7:'July',  8:'August',  9:'September',  10:'October',  11:'November',  12:'December'}\n",
    "for m in range(1,13):\n",
    "    mogo = pd.DataFrame()\n",
    "    for s in stations:\n",
    "        lineo = Station_stats[s][Station_stats[s]['Month'] == m]\n",
    "        mogo = pd.concat([mogo, lineo], axis=0)\n",
    "    Monthly_stats[month_key[m]] = mogo\n",
    "\n",
    "# print out a daily streamflow CSV for each station\n",
    "for i in All_stations.keys():\n",
    "    All_stations[i].to_csv(os.path.join(workspace, \"{}_Daily_streamflowCFS_{}_draft.csv\".format(i, str(date.today()))))\n",
    "    \n",
    "# print out a monthly average streamflow CSV for each month\n",
    "for m in Monthly_stats.keys():\n",
    "    Monthly_stats[m].to_csv(os.path.join(workspace, \"{}_MonthlyAve_flowCFS_{}_draft.csv\".format(m, str(date.today()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
